{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "B2G6wonI8KTd"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dzeek/DeepLearningLesions/blob/main/Copy_of_Case_1_Convolutional_Neural_Network_for_skin_lesion_classification_understanding_the_fundamentals_through_hands_on_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TwkczAD7m0c"
      },
      "source": [
        "# Case 1: Example of classification of images of skin lesions as bening or malignant\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2G6wonI8KTd"
      },
      "source": [
        "## 0. Imports\n",
        "---\n",
        "Execute this cell to load all the necessary libraries to run the code. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKJqUyqf7vZy"
      },
      "source": [
        "!pip install q keras==2.2.4\n",
        "%tensorflow_version 1.15.0\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.models import  Model\n",
        "from keras.optimizers import SGD\n",
        "!pip install keras_lr_multiplier\n",
        "from keras_lr_multiplier import LRMultiplier #this package needs to be installed\n",
        "from keras import regularizers\n",
        "from keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint, Callback\n",
        "from math import ceil\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
        "from matplotlib import pyplot\n",
        "from keras import models\n",
        "from PIL import ImageFile\n",
        "from PIL import Image\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# ISIC Dataset download\n",
        "import csv\n",
        "import urllib\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Split datasets\n",
        "import random\n",
        "random.seed(54)\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "# Tensorflow\n",
        "import tensorflow as tf\n",
        "\n",
        "# Images\n",
        "import matplotlib.image as mpimg\n",
        "from skimage.exposure import rescale_intensity\n",
        "import argparse\n",
        "import cv2\n",
        "\n",
        "! pip install -U scikit-image\n",
        "\n",
        "#@title ISIC Api\n",
        "import requests\n",
        "\n",
        "class ISICApi(object):\n",
        "    def __init__(self, hostname='https://isic-archive.com',\n",
        "                 username=None, password=None):\n",
        "        self.baseUrl = f'{hostname}/api/v1'\n",
        "        self.authToken = None\n",
        "\n",
        "        if username is not None:\n",
        "            if password is None:\n",
        "                password = input(f'Password for user \"{username}\":')\n",
        "            self.authToken = self._login(username, password)\n",
        "\n",
        "    def _makeUrl(self, endpoint):\n",
        "        return f'{self.baseUrl}/{endpoint}'\n",
        "\n",
        "    def _login(self, username, password):\n",
        "        authResponse = requests.get(\n",
        "            self._makeUrl('user/authentication'),\n",
        "            auth=(username, password)\n",
        "        )\n",
        "        if not authResponse.ok:\n",
        "            raise Exception(f'Login error: {authResponse.json()[\"message\"]}')\n",
        "\n",
        "        authToken = authResponse.json()['authToken']['token']\n",
        "        return authToken\n",
        "\n",
        "    def get(self, endpoint):\n",
        "        url = self._makeUrl(endpoint)\n",
        "        headers = {'Girder-Token': self.authToken} if self.authToken else None\n",
        "        return requests.get(url, headers=headers)\n",
        "\n",
        "    def getJson(self, endpoint):\n",
        "        return self.get(endpoint).json()\n",
        "\n",
        "    def getJsonList(self, endpoint):\n",
        "        endpoint += '&' if '?' in endpoint else '?'\n",
        "        LIMIT = 50\n",
        "        offset = 0\n",
        "        while True:\n",
        "            resp = self.get(\n",
        "                f'{endpoint}limit={LIMIT:d}&offset={offset:d}'\n",
        "            ).json()\n",
        "            if not resp:\n",
        "                break\n",
        "            for elem in resp:\n",
        "                yield elem\n",
        "            offset += LIMIT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4EPmcvB8dVJ"
      },
      "source": [
        "## 1. Loading and organizing the dataset\n",
        "---\n",
        "We will use the image dataset from the [ISIC Archive](https://www.isic-archive.com/#!/topWithHeader/wideContentTop/main), a public repository of images for the teaching, the development, and the testing of automated diagnostic systems.\n",
        "\n",
        "To run the example, it is necessary to obtain the images and upload them on your Colab account \"Files\". For this, you first need to create a (free) account on the ISIC archive. Once you get your registration, insert your username and password into the cell below. Also, select the number of images per class that you want to upload.\n",
        "\n",
        "The images will be temporarily saved in a new directory named \"ISIC_2019\" and organized in subfolders for the training/ validation/ test according to their classes. \n",
        "\n",
        "For the intuitive understanding of the steps of this example, it is sufficient to use a small number of images (~50-100 per class) that guarantee fast computation. However, for the training and fine-tuning of the CNN, larger dataset must be used with a corresponding slow down of the computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdEIwgzDA5SU",
        "cellView": "form"
      },
      "source": [
        "#@title 1.1 Access the dataset  { form-width: \"30%\" }\n",
        "ISIC_username = \"username\" #@param{type: 'string'}\n",
        "ISIC_password = \"password\" #@param{type: 'string'}\n",
        "image_num =    50#@param{type: 'integer'}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9hcg4g4yy23",
        "cellView": "form"
      },
      "source": [
        "#@title 1.2 Get metadata and create folders  { form-width: \"30%\" }\n",
        "\n",
        "#@markdown Delete old folders (if exist) { form-width: \"10%\" }\n",
        "!rm -rf \"ISIC_2019\"\n",
        "!rm -rf \"Toy_img\"\n",
        "!rm -rf \"models_and_metadata_isic_images\"\n",
        "\n",
        "#@markdown Get image list and clone GitHub folder with metadata and trained CNNs { form-width: \"10%\" }\n",
        "\n",
        "api = ISICApi(username=ISIC_username, password=ISIC_password)\n",
        "\n",
        "Json_name = \"image?limit=1000000&offset=0&sort=name\"\n",
        "\n",
        "imageList = api.getJson(Json_name)\n",
        "\n",
        "if os.path.exists(os.path.join(os.getcwd(), 'metadata_isic_images', 'imagedata.csv'))==False:\n",
        "  #! git clone https://github.com/SergioNoe/sergio_isic_images metadata_isic_images\n",
        "  ! git clone https://github.com/qubilab/CNN-for-skin-lesion-classification models_and_metadata_isic_images\n",
        "\n",
        "#@markdown Create a new folder named ISIC_2019 containing training, validation and test subfolders { form-width: \"10%\" }\n",
        "newdir=\"ISIC_2019\"\n",
        "!mkdir $newdir\n",
        "\n",
        "newdir=\"ISIC_2019/training\"\n",
        "!mkdir $newdir\n",
        "\n",
        "newdir=\"ISIC_2019/training/benign\"\n",
        "!mkdir $newdir\n",
        "\n",
        "newdir=\"ISIC_2019/training/malignant\"\n",
        "!mkdir $newdir\n",
        "\n",
        "newdir=\"ISIC_2019/validation\"\n",
        "!mkdir $newdir\n",
        "\n",
        "newdir=\"ISIC_2019/validation/benign\"\n",
        "!mkdir $newdir\n",
        "\n",
        "newdir=\"ISIC_2019/validation/malignant\"\n",
        "!mkdir $newdir\n",
        "\n",
        "newdir=\"ISIC_2019/test\"\n",
        "!mkdir $newdir\n",
        "\n",
        "newdir=\"ISIC_2019/test/benign\"\n",
        "!mkdir $newdir\n",
        "\n",
        "newdir=\"ISIC_2019/test/malignant\"\n",
        "!mkdir $newdir\n",
        "\n",
        "newdir=\"Toy_img\"\n",
        "!mkdir $newdir\n",
        "\n",
        "newdir=\"Toy_img/Toy\"\n",
        "!mkdir $newdir\n",
        "\n",
        "newdir=\"Toy_img/Toy_RGB\"\n",
        "!mkdir $newdir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqMoo0qfdlJi",
        "cellView": "form"
      },
      "source": [
        "#@title 1.3 Download the images and distribute them among the datasets  { form-width: \"30%\" }\n",
        "#@markdown 1. Set percentage of images to be used for Training (default = 65%)\n",
        "training  =  65  #@param{type: 'integer'} \n",
        "#@markdown 2. Set percentage of images to be used for Validation (default = 20%)\n",
        "validation = 20 #@param{type: 'integer'}\n",
        "#@markdown 3. Set percentage of images to be used for Test (default = 15%)\n",
        "test = 15  #@param{type: 'integer'}\n",
        "\n",
        "csvf = pd.read_csv(\"/content/models_and_metadata_isic_images/metadata/imagedata.csv\")\n",
        "\n",
        "csvf_BE = csvf[csvf[\"benign_malignant\"] == \"benign\"]\n",
        "csvf_MAL = csvf[csvf[\"benign_malignant\"] == \"malignant\"]\n",
        "\n",
        "probtrain =  training/100\n",
        "probvalidation = validation/100\n",
        "\n",
        "def download_image_dataset(subset, lesion, images, image_to_download):\n",
        "  \n",
        "  print(\"Downloading images of the class labeled as \" + lesion + \".\")\n",
        "\n",
        "  savePath = \"ISIC_2019/\"\n",
        "  savePathTrain = os.path.join(savePath, \"training\", lesion)\n",
        "  savePathValidation = os.path.join(savePath, \"validation\", lesion)\n",
        "  savePathTest = os.path.join(savePath, \"test\", lesion)\n",
        " \n",
        "  indexes1 = list(range(0,images))\n",
        "  index = random.sample(indexes1, min([images,image_to_download]))\n",
        "\n",
        "  print(\"Found \" + str(min([images,image_to_download])) + \" of the \" +str(image_to_download) + \" required images.\")\n",
        "\n",
        "\n",
        "  k=0\n",
        "  for index_1 in index:\n",
        "    key = subset[\"isic_id\"].keys()[index_1]\n",
        "    name_img = subset[\"isic_id\"][key]\n",
        "    img_id = imageList[key][\"_id\"]\n",
        "    imageFileResp = api.get('image/%s/download' % img_id)\n",
        "    imageFileResp.raise_for_status()\n",
        "\n",
        "    if k <= ceil(len(index)*probtrain):\n",
        "      imageFileOutputPath = os.path.join(savePathTrain, '%s.jpg' % name_img)\n",
        "    elif k > ceil(len(index)*probtrain) and k <= ceil((len(index)*probvalidation)+(len(index)*probtrain)):\n",
        "      imageFileOutputPath = os.path.join(savePathValidation, '%s.jpg' % name_img)\n",
        "    else:\n",
        "      imageFileOutputPath = os.path.join(savePathTest, '%s.jpg' % name_img)\n",
        "\n",
        "    with open(imageFileOutputPath, 'wb') as imageFileOutputStream:\n",
        "      for chunk in imageFileResp:\n",
        "        imageFileOutputStream.write(chunk)\n",
        "    k+=1\n",
        "\n",
        "#num_images = len(csvf_BE)\n",
        "#if len(csvf_MAL)<num_images: num_images=len(csvf_MAL)\n",
        "#download_image_dataset(csvf_BE, \"benign\", num_images, image_num)\n",
        "#download_image_dataset(csvf_MAL, \"malignant\", num_images, image_num)\n",
        "\n",
        "download_image_dataset(csvf_BE, \"benign\", len(csvf_BE), image_num)\n",
        "download_image_dataset(csvf_MAL, \"malignant\", len(csvf_MAL), image_num)\n",
        "\n",
        "#print(\"Downloading toy image.\")\n",
        "#savePathToy = \"Toy_img/Toy\"\n",
        "#img_id = '5436e3abbae478396759f0cf'\n",
        "#imageFileResp = api.get('image/%s/download' % img_id)\n",
        "#imageFileResp.raise_for_status()\n",
        "#imageFileOutputPath = os.path.join(savePathToy, '%s.jpg' % \"ISIC_0000000\")\n",
        "#with open(imageFileOutputPath, 'wb') as imageFileOutputStream:\n",
        "#  for chunk in imageFileResp:\n",
        "#    imageFileOutputStream.write(chunk)\n",
        "\n",
        "print(\"Done!\")\n",
        "\n",
        "# Defining paths to training, validation and test datasets { form-width: \"10%\" }\n",
        "# change the destination path accordingly\n",
        "#mainpath = '../ISIC_2019'\n",
        "mainpath = \"/content/ISIC_2019\"\n",
        "# in this example, we assume that the training, validation and test datsets have been already split in different folders\n",
        "train_fold = os.path.join(mainpath, 'training')\n",
        "validation_fold = os.path.join(mainpath, 'validation')\n",
        "test_fold = os.path.join(mainpath, 'test')\n",
        "\n",
        "# Define the input size of the network.\n",
        "# We will use ResNet-50 that accepts inputs 224x224x3\n",
        "inputSize= [224, 224, 3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDChL8eU9V3h"
      },
      "source": [
        "## 2. Understanding images and convolution\n",
        "---\n",
        "Visualize images and apply convolution kernels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3C_05yi-o5G",
        "cellView": "form"
      },
      "source": [
        "#@title 2.1 Select and visualize a random image\n",
        "#@markdown 1. Choose a the dataset:\n",
        "dataset = \"test\" #@param [\"training\", \"validation\", \"test\"]\n",
        "#@markdown 2. Choose a class:\n",
        "class_label = \"benign\" #@param [\"benign\", \"malignant\"]\n",
        "\n",
        "inputSize=[224,224,3]\n",
        "filenames=os.listdir(os.path.join('ISIC_2019',dataset,class_label)) \n",
        "rand_ind=np.random.randint(len(filenames))\n",
        "\n",
        "fig_0 = pyplot.figure(1, figsize=(5, 5))\n",
        "chart_1 = fig_0.add_subplot()\n",
        "chart_1.title.set_text('Sample Image')\n",
        "\n",
        "img = Image.open(os.path.join('ISIC_2019',dataset,class_label,filenames[rand_ind]))\n",
        "img=img.resize(inputSize[0:2])\n",
        "chart_1.imshow(img)\n",
        "data = img.getdata()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPIh0auK8X9C",
        "cellView": "form"
      },
      "source": [
        "#@title 2.2 Images, layers and pixels\n",
        "\n",
        "#@markdown An RGB image consists of three layers of pixels, where layers represents the red, green and blue component, respectively.\n",
        "\n",
        "# #@markdown In the following, we will use ResNet-50, a neural network with 50 layers that allows to classify images according to different classes associated to the image content. In our example, we will use it to classify skin lesions.\n",
        "\n",
        "# #@markdown ResNet-50 requires images with 224-by-224 pixels in RGB color coding.\n",
        "\n",
        "\n",
        "# Image PIL y matrix\n",
        "\n",
        "#fig_0 = pyplot.figure(1, figsize=(5, 5))\n",
        "#chart_1 = fig_0.add_subplot()\n",
        "#chart_1.title.set_text('Original Image')\n",
        "\n",
        "#img = Image.open(\"Toy_img/Toy/ISIC_0000000.jpg\")\n",
        "#img=img.resize(inputSize[0:2])\n",
        "#chart_1.imshow(img)\n",
        "#data = img.getdata()\n",
        "\n",
        "img = Image.open(os.path.join('ISIC_2019',dataset,class_label,filenames[rand_ind]))\n",
        "img=img.resize(inputSize[0:2])\n",
        "\n",
        "fig_1 = pyplot.figure(2, figsize=(16, 5))\n",
        "\n",
        "chart_2 = fig_1.add_subplot(131)\n",
        "chart_3 = fig_1.add_subplot(132)\n",
        "chart_4 = fig_1.add_subplot(133)\n",
        "\n",
        "\n",
        "chart_2.title.set_text('Red Channel')\n",
        "chart_3.title.set_text('Green Channel')\n",
        "chart_4.title.set_text('Blue Channel')\n",
        "\n",
        "#@markdown Each pixels contains the numeric information about the intensity of each specific color.\n",
        "\n",
        "# Suppress specific channels (e.g. (255, 120, 65) -> (0, 120, 0) for g)\n",
        "r = [(d[0], 0, 0) for d in data]\n",
        "g = [(0, d[1], 0) for d in data]\n",
        "b = [(0, 0, d[2]) for d in data]\n",
        "\n",
        "img.putdata(r)\n",
        "img.save('Toy_img/Toy_RGB/r.png')\n",
        "imgr = Image.open('Toy_img/Toy_RGB/r.png')\n",
        "chart_2.imshow(imgr)\n",
        "\n",
        "img.putdata(g)\n",
        "img.save('Toy_img/Toy_RGB/g.png')\n",
        "imgg = Image.open('Toy_img/Toy_RGB/g.png')\n",
        "chart_3.imshow(imgg)\n",
        "\n",
        "img.putdata(b)\n",
        "img.save('Toy_img/Toy_RGB/b.png')\n",
        "imgb = Image.open('Toy_img/Toy_RGB/b.png')\n",
        "chart_4.imshow(imgb)\n",
        "\n",
        "pyplot.show()\n",
        "\n",
        "#img = Image.open(\"Toy_img/Toy/ISIC_0000000.jpg\")\n",
        "#img=img.resize(inputSize[0:2])\n",
        "#img = np.array(img)\n",
        "\n",
        "img = Image.open(os.path.join('ISIC_2019',dataset,class_label,filenames[rand_ind]))\n",
        "img=img.resize(inputSize[0:2])\n",
        "img = np.array(img)\n",
        "\n",
        "# matrix plot\n",
        "fig, (ax1, ax2, ax3) = pyplot.subplots(1,3)\n",
        "fig.set_size_inches(16, 5)\n",
        "\n",
        "min_val_i, min_val_j = 24, 84\n",
        "size=10\n",
        "intersection_matrix = img[:,:,0]\n",
        "\n",
        "ax1.imshow(imgr) #\n",
        "ax1.set_xlim([min_val_i,min_val_i+size])\n",
        "ax1.set_ylim([min_val_j,min_val_j+size])\n",
        "\n",
        "\n",
        "for i in range(min_val_i+1,min_val_i+size):\n",
        "    for j in range(min_val_j+1,min_val_j+size):\n",
        "        c = intersection_matrix[j,i]\n",
        "        ax1.text(i, j, str(c), va='center', ha='center', color='w')\n",
        "\n",
        "ax1.title.set_text('Zoom-in of a region of the red channel \\n with pixel values')\n",
        "\n",
        "\n",
        "intersection_matrix = img[:,:,1]\n",
        "\n",
        "ax2.imshow(imgg) #\n",
        "ax2.set_xlim([min_val_i,min_val_i+size])\n",
        "ax2.set_ylim([min_val_j,min_val_j+size])\n",
        "\n",
        "for i in range(min_val_i+1,min_val_i+size):\n",
        "    for j in range(min_val_j+1,min_val_j+size):\n",
        "        c = intersection_matrix[j,i]\n",
        "        ax2.text(i, j, str(c), va='center', ha='center', color='w')\n",
        "\n",
        "ax2.title.set_text('Zoom-in of a region of the green channel \\n with pixel values')\n",
        "\n",
        "intersection_matrix = img[:,:,2]\n",
        "\n",
        "ax3.imshow(imgb) #\n",
        "ax3.set_xlim([min_val_i,min_val_i+size])\n",
        "ax3.set_ylim([min_val_j,min_val_j+size])\n",
        "\n",
        "for i in range(min_val_i+1,min_val_i+size):\n",
        "    for j in range(min_val_j+1,min_val_j+size):\n",
        "        c = intersection_matrix[j,i]\n",
        "        ax3.text(i, j, str(c), va='center', ha='center', color='w')\n",
        "\n",
        "ax3.title.set_text('Zoom-in of a region of the blue channel \\n with pixel values')\n",
        "\n",
        "ax1.invert_yaxis()\n",
        "ax2.invert_yaxis()\n",
        "ax3.invert_yaxis()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBXh60d9Pngy",
        "cellView": "form"
      },
      "source": [
        "#@title 2.3 Convolution kernels\n",
        "\n",
        "#@markdown Example of convolution kernels\n",
        "\n",
        "smallBlur = np.ones((7, 7), dtype=\"float\") * (1.0 / (7 * 7 ))\n",
        "largeBlur = np.ones((15, 15), dtype=\"float\") * (1.0 / (15 * 15 ))\n",
        "sharpen = np.array(([0, -1, 0],[-1, 5, -1],[0, -1, 0]), dtype=\"int\")\n",
        "laplacian = np.array(([0, 1, 0],[1, -4, 1],[0, 1, 0]), dtype=\"int\")\n",
        "sobelX = np.array(([-1, 0, 1],[-2, 0, 2],[-1, 0, 1]), dtype=\"int\")\n",
        "sobelY = np.array(([-1, -2, -1],[0, 0, 0],[1, 2, 1]), dtype=\"int\")\n",
        "\n",
        "fig_2,((ax1, ax2, ax3), (ax4, ax5, ax6)) = pyplot.subplots(2,3)\n",
        "fig_2.set_size_inches(16, 12)\n",
        "fig_2.suptitle(\"Convolution kernels\", fontsize=16, y=0.92, weight='bold')\n",
        "\n",
        "ax1.title.set_text(\"Small average kernel\")\n",
        "ax2.title.set_text(\"Large average kernel\")\n",
        "ax3.title.set_text(\"Sharpen kernel\")\n",
        "ax4.title.set_text(\"Laplacian kernel\")\n",
        "ax5.title.set_text(\"SobelX kernel\")\n",
        "ax6.title.set_text(\"SobelY kernel\")\n",
        "\n",
        "smallBlur = np.ones((7, 7), dtype=\"float\") * (1.0 / (7 * 7))\n",
        "ax1.imshow(smallBlur, cmap='gray')\n",
        "for i in range(smallBlur.shape[1]):\n",
        "    for j in range(smallBlur.shape[0]):\n",
        "        c = np.round(smallBlur[j,i],2)\n",
        "        ax1.text(i, j, str(c), va='center', ha='center', color=\"r\")\n",
        "\n",
        "largeBlur = np.ones((15, 15), dtype=\"float\") * (1.0 / (15 * 15))\n",
        "ax2.imshow(largeBlur, cmap='gray')\n",
        "for i in range(largeBlur.shape[1]):\n",
        "    for j in range(largeBlur.shape[0]):\n",
        "        c = np.round(largeBlur[j,i],3)\n",
        "        ax2.text(i, j, str(c), va='center', ha='center', color=\"r\", fontsize=6)\n",
        "\n",
        "ax3.imshow(sharpen, cmap='gray')\n",
        "for i in range(sharpen.shape[1]):\n",
        "    for j in range(sharpen.shape[0]):\n",
        "        c = sharpen[j,i]\n",
        "        ax3.text(i, j, str(c), va='center', ha='center', color='r')\n",
        "\n",
        "ax4.imshow(laplacian, cmap='gray')\n",
        "for i in range(laplacian.shape[1]):\n",
        "    for j in range(laplacian.shape[0]):\n",
        "        c = laplacian[j,i]\n",
        "        ax4.text(i, j, str(c), va='center', ha='center', color='r')\n",
        "\n",
        "ax5.imshow(sobelX, cmap='gray')\n",
        "for i in range(sobelX.shape[1]):\n",
        "    for j in range(sobelX.shape[0]):\n",
        "        c = sobelX[j,i]\n",
        "        ax5.text(i, j, str(c), va='center', ha='center', color='r')\n",
        "\n",
        "ax6.imshow(sobelY, cmap='gray')\n",
        "for i in range(sobelY.shape[1]):\n",
        "    for j in range(sobelY.shape[0]):\n",
        "        c = sobelY[j,i]\n",
        "        ax6.text(i, j, str(c), va='center', ha='center', color='r')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "gAAN_njmA5DZ"
      },
      "source": [
        "#@title 2.4 Results of convolution\n",
        "\n",
        "def convolve(image0, kernel):\n",
        "\t# grab the spatial dimensions of the image, along with\n",
        "\t# the spatial dimensions of the kernel\n",
        "\t(iH, iW) = image0.shape[:2]\n",
        "\t(kH, kW) = kernel.shape[:2]\n",
        "\t# allocate memory for the output image, taking care to\n",
        "\t# \"pad\" the borders of the input image so the spatial\n",
        "\t# size (i.e., width and height) are not reduced\n",
        "\tpad = (kW - 1) // 2\n",
        "\t\n",
        "\toutput3 = np.zeros((iH, iW,3), dtype=\"float32\")\n",
        "\t# loop over the input image, \"sliding\" the kernel across\n",
        "\t# each (x, y)-coordinate from left-to-right and top to\n",
        "\t# bottom\n",
        "\tfor z in range(3):\n",
        "\t\toutput = np.zeros((iH, iW), dtype=\"float32\")\n",
        "\t\timage = cv2.copyMakeBorder(image0[:,:,z], pad, pad, pad, pad,\n",
        "\t\tcv2.BORDER_REPLICATE)\n",
        "\t\t#image=image0[:,:,z]\n",
        "\t\tfor y in np.arange(pad, iH + pad):\n",
        "\t\t\tfor x in np.arange(pad, iW + pad):\n",
        "\t\t\t\t# extract the ROI of the image by extracting the\n",
        "\t\t\t\t# *center* region of the current (x, y)-coordinates\n",
        "\t\t\t\t# dimensions\n",
        "\t\t\t\troi = image[y - pad:y + pad + 1, x - pad:x + pad + 1]\n",
        "\t\t\t\t# perform the actual convolution by taking the\n",
        "\t\t\t\t# element-wise multiplicate between the ROI and\n",
        "\t\t\t\t# the kernel, then summing the matrix\n",
        "\t\t\t\tk = (roi * kernel).sum()\n",
        "\t\t\t\t# store the convolved value in the output (x,y)-\n",
        "\t\t\t\t# coordinate of the output image\n",
        "\t\t\t\toutput[y - pad, x - pad] = k\n",
        "\n",
        "\t# rescale the output image to be in the range [0, 255]\n",
        "\t\t#output3[:,:,z] = rescale_intensity(output, in_range=(0, 255))\n",
        "\t\toutput3[:,:,z] = np.round(output,0)\n",
        "\toutput3 = (output3).astype(\"uint8\")\n",
        "\t# return the output image\n",
        "\treturn output3\n",
        "\n",
        "#@markdown Effect of image convolution using different kernels\n",
        "\n",
        "#fig_0 = pyplot.figure(1, figsize=(20, 5))\n",
        "#chart_1 = fig_0.add_subplot()\n",
        "#chart_1.title.set_text(\"Original Image\")\n",
        "img = Image.open(os.path.join('ISIC_2019',dataset,class_label,filenames[rand_ind]))\n",
        "img=img.resize(inputSize[0:2])\n",
        "#chart_1.imshow(img)\n",
        "img = np.array(img)\n",
        "\n",
        "small_kernel = convolve(img, smallBlur)\n",
        "large_kernel = convolve(img, largeBlur)\n",
        "sharpen_kernel = convolve(img, sharpen)\n",
        "laplacian_kernel = convolve(img, laplacian)\n",
        "sobelX_kernel = convolve(img, sobelX)\n",
        "sobelY_kernel = convolve(img, sobelY)\n",
        "\n",
        "\n",
        "fig_1,((bx1, bx2, bx3), (bx4, bx5, bx6)) = pyplot.subplots(2,3)\n",
        "fig_1.set_size_inches(16, 12)\n",
        "fig_1.suptitle(\"Image convolution\", fontsize=16, y=0.92, weight='bold')\n",
        "\n",
        "bx1.imshow(small_kernel)\n",
        "bx2.imshow(large_kernel)\n",
        "bx3.imshow(sharpen_kernel)\n",
        "bx4.imshow(laplacian_kernel)\n",
        "bx5.imshow(sobelX_kernel)\n",
        "bx6.imshow(sobelY_kernel)\n",
        "\n",
        "bx1.title.set_text(\"Small average kernel\")\n",
        "bx2.title.set_text(\"Large average kernel\")\n",
        "bx3.title.set_text(\"Sharpen kernel\")\n",
        "bx4.title.set_text(\"Laplacian kernel\")\n",
        "bx5.title.set_text(\"SobelX kernel\")\n",
        "bx6.title.set_text(\"SobelY kernel\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNyFBFz98wIL"
      },
      "source": [
        "## 3. Data augmentation\n",
        "---\n",
        "By applying transformations, such as reflection along the x or y axis, translations, and rotations, to the images in the original dataset we can improves generalization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4n1CbXe8YiM",
        "cellView": "form"
      },
      "source": [
        "#@title 3.1 Define transformations and apply them to the training set{ form-width: \"30%\" }\n",
        "\n",
        "# Create image generator with for the training dataset\n",
        "# Use data augmentation to prevent the memorization of image features\n",
        "# Here we apply reflection (both x and y axis), translation (between -30 and 30 pixels,\n",
        "# and rotation (up to 45 degrees).\n",
        "\n",
        "datagenTrain = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        samplewise_center=False,\n",
        "        samplewise_std_normalization=False,\n",
        "        rotation_range= 45,\n",
        "        width_shift_range=[-30, 30],\n",
        "        height_shift_range=[-30, 30],\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "        fill_mode='nearest')\n",
        "\n",
        "# create image generator without augmentation for the validation and test dataset\n",
        "datagenValTest = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        samplewise_center=False,\n",
        "        samplewise_std_normalization=False)\n",
        "\n",
        "## Create datasets, keeping the same organization in subfolders. The folder name correspond to the class label\n",
        "\n",
        "Train_set = datagenTrain.flow_from_directory(train_fold, batch_size=32 ,target_size = inputSize[0:2], class_mode=\"categorical\")\n",
        "Validation_set = datagenValTest.flow_from_directory(validation_fold, batch_size=32 ,target_size = inputSize[0:2], class_mode=\"categorical\")\n",
        "Test_set = datagenValTest.flow_from_directory(test_fold, target_size = inputSize[0:2])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMWQcINhKwAS",
        "cellView": "form"
      },
      "source": [
        "#@title 3.2 Select the number of augmented images to show { form-width: \"30%\" }\n",
        "\n",
        "##@title Original image { form-width: \"30%\" }\n",
        "#img = Image.open(os.path.join('ISIC_2019',dataset,class_label,filenames[rand_ind]))\n",
        "#img=img.resize(inputSize[0:2])\n",
        "#pyplot.imshow(img)\n",
        "#pyplot.show()\n",
        "!rm -rf \"Toy_img\"\n",
        "\n",
        "newdir=\"Toy_img\"\n",
        "!mkdir $newdir\n",
        "\n",
        "newdir=\"Toy_img/Toy\"\n",
        "!mkdir $newdir\n",
        "\n",
        "original=os.path.join('ISIC_2019',dataset,class_label,filenames[rand_ind])\n",
        "target=os.path.join('Toy_img','Toy',filenames[rand_ind])\n",
        "shutil.copyfile(original, target)\n",
        "\n",
        "Toy_set = datagenTrain.flow_from_directory(\"/content/Toy_img/\", batch_size=32 ,target_size = inputSize[0:2])\n",
        "\n",
        "img_generated =  24#@param{type: 'integer'}\n",
        "\n",
        "#@markdown We show an example of the augmentation result, using the random image selected above, being transformed accordingly.\n",
        "\n",
        "# Observe result of an image after execute the augmentation\n",
        "\n",
        "\n",
        "sub_plots = list(range(1,img_generated+1))\n",
        "fig = pyplot.figure(figsize=(50, 50))\n",
        "\n",
        "for img in range(0,img_generated):\n",
        "  x,y = Toy_set.next()\n",
        "  image = x[0]\n",
        "  sub = fig.add_subplot(8,8, sub_plots[img]) #change for optimal display based on number of images\n",
        "  sub.imshow(image)\n",
        "pyplot.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGIbs3bSBO2q"
      },
      "source": [
        "## 4. The CNN: ResNet-50 (*Optional*)\n",
        "---\n",
        "*If you don't want to train the network, skip this section and go to:* **7. Load a trained network**\n",
        "\n",
        "Download the pre-trained ResNet-50 architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "NnDqyLsr9rWb"
      },
      "source": [
        "#@title 4.1 Pre-processing and augmentation\n",
        "#@markdown Define data-preprocessing and augmentation for fine-tuning\n",
        "#\n",
        "#@markdown Set the batch size, the number of images used to optimize the network at each iteration\n",
        "batch_size= 6 #@param {type:\"integer\"}\n",
        "datagenTrain = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "        rotation_range= 90,\n",
        "        width_shift_range=[-30, 30],\n",
        "        height_shift_range=[-30, 30],\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "        fill_mode='nearest')\n",
        "\n",
        "datagenValTest = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "#\n",
        "Train_set =   datagenTrain.flow_from_directory(train_fold,\n",
        "                                batch_size=batch_size,\n",
        "                                target_size = inputSize[0:2],\n",
        "                                class_mode=\"categorical\")\n",
        "Validation_set = datagenValTest.flow_from_directory(validation_fold,\n",
        "                                batch_size=batch_size,\n",
        "                                target_size = inputSize[0:2],\n",
        "                                class_mode=\"categorical\")\n",
        "\n",
        "Test_set = datagenValTest.flow_from_directory(test_fold,\n",
        "                                target_size = inputSize[0:2],\n",
        "                                class_mode=\"categorical\")\n",
        "\n",
        "from collections import Counter\n",
        "counter = Counter(Train_set.classes)\n",
        "max_val = float(max(counter.values()))\n",
        "class_weights = {class_id : max_val/num_images for class_id, num_images in counter.items()}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9Y8IOzB81Tz",
        "cellView": "form"
      },
      "source": [
        "#@title 4.2 Load the pre-trained ResNet-50 and change the final layers\n",
        "\n",
        "#@markdown  Download the pre-trained ResNet-50 with weights obtained from training on the [Imagenet](http://image-net.org/index) dataset\n",
        "#load the pre-trained Network, weights obtained from training on ImageNet dataset\n",
        "#@markdown To classify lesions as benign/malignant, we need to substitute the final layers (fc1000) of the ResNet-50 to reduce the number of ouputs from 1000 to 2.\n",
        "# The new fully connected layer will have a number of classes equal to Train_data.num_classes (2)\n",
        "\n",
        "model_ResNet50 = ResNet50(include_top=False,pooling = 'avg', weights='imagenet', input_shape=(224,224,3))\n",
        "x = model_ResNet50.output\n",
        "fc2 = Dense(Train_set.num_classes, activation='softmax', name = 'fc2')(x)\n",
        "model = Model(inputs=model_ResNet50.input, outputs=fc2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmE-pK-V8YnA",
        "cellView": "form"
      },
      "source": [
        "#@title 4.3 Visualize the final architecture\n",
        "#@markdown Notice the change of number of classes of the last dense layer.\n",
        "# this is the final architecture\n",
        "#model = Model(inputs=model_ResNet50.input, outputs=predict)\n",
        "print(model.summary())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lji1baA-BhZ"
      },
      "source": [
        "## 5. Hyperparameters (*Optional*)\n",
        "---\n",
        "*If you don't want to train the network, skip this section and go to:* **7. Load a trained network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv-Bpotk8Yrt",
        "cellView": "form"
      },
      "source": [
        "#@title 5.1 Learning rate { form-width: \"30%\" }\n",
        "#@markdown Define the base learning rate and the momentum of the optimizer of the network. \n",
        "learning_rate = 0.0001 #@param {type:\"number\"}\n",
        "momentum = 0.9 #@param {type:\"number\"}\n",
        "#@markdown One can also change the learning rate of some layer, for example the prediction layer, with respect to the base \n",
        "#@markdown learning rate, by multiplying it for a factor: \n",
        "m_factor = 10 #@param {type:\"integer\"}\n",
        "# Before starting the training, there are a few hyperparameters to set\n",
        "# First, we define the base learning rate lr and the momentum\n",
        "# Here we can also change the learning rate of some layer, for example the fully connected layer\n",
        "# with respect to the base learning rate (1 means the same as the base one, 10 means 10x faster)\n",
        "opt =LRMultiplier(SGD(lr=learning_rate, momentum=momentum, nesterov=False), {'fc2': m_factor} ) #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TB9Yxjc8Ywo",
        "cellView": "form"
      },
      "source": [
        "#@title 5.2 Variable learning rate and stopping condition { form-width: \"30%\" }\n",
        "def lr_scheduler(epoch, lr): \n",
        "  #@markdown  Reduce the learning rate by a factor \n",
        "  factor =  0.1 #@param {type:\"number\"}\n",
        "  #@markdown every\n",
        "  step = 5 #@param {type:\"integer\"}\n",
        "  #@markdown where each step corresponds to an epoch of training. { form-width: \"10%\" }\n",
        "  if epoch % step == 0 and epoch:\n",
        "      return lr * factor\n",
        "  return lr\n",
        "\n",
        "#@markdown Stop the training if the loss function doesn't decrease for \n",
        "patience =  10#@param {type:\"integer\"} \n",
        "#@markdown epochs. { form-width: \"10%\" }\n",
        "\n",
        "# we will change the learning rate piecewise every 5 epochs by reducing it  by a factor 10\n",
        "#def lr_scheduler(epoch, lr): #reduce learning rate 0.1 after 5 epochs\n",
        "#    factor = factor\n",
        "#    step = step\n",
        "#   if epoch % step == 0 and epoch:\n",
        "#        return lr * factor\n",
        "#    return lr\n",
        "Learning_rate_decrease = LearningRateScheduler(lr_scheduler, verbose=1)\n",
        "\n",
        "# stop the optimization if the loss function doesn't decrease for # iter\n",
        "stopping = EarlyStopping(monitor='val_loss', patience=patience)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUP7Upmm81WQ",
        "cellView": "form"
      },
      "source": [
        "#@title 5.3 (optional) Freezing layers\n",
        "\n",
        "#@markdown We can choose if a layer of ResNet-50 will be trained or not using the code line from below. In this example, we choose to \"freeze\", or maintain the weights obtained on the Imagenet dataset.\n",
        "\n",
        "for layer in range(0,20):\n",
        "  model.layers[layer].trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gudj3ukZ8YuM",
        "cellView": "form"
      },
      "source": [
        "#@title 5.4 Loss function and metrics { form-width: \"30%\" }\n",
        "#@markdown Select the **categorical crossentropy** as the loss function and the **accuracy** as the metrics. { form-width: \"10%\" }\n",
        "\n",
        "# define loss function and metrics\n",
        "model.compile(optimizer=opt, \n",
        "              loss = 'categorical_crossentropy', \n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wGZqF-U8Y1N",
        "cellView": "form"
      },
      "source": [
        "#@title 5.5 Model saving and performance visualization { form-width: \"30%\" }\n",
        "#@markdown Check the loss function value at the end of each iteration, save the model when performance improves. \n",
        "\n",
        "#Save best model\n",
        "with open(\"./model_architecture.json\", 'w') as f:\n",
        "        f.write(model.to_json())\n",
        "checkpointer = ModelCheckpoint(\"./model_weights.h5\", verbose=0,monitor='val_loss', mode='min', save_best_only=True, save_weights_only=True)\n",
        "\n",
        "\n",
        "#@markdown At every iteration/epoch, show losses and accuracy for training and validation datasets.\n",
        "\n",
        "class GraphCallback(Callback):\n",
        "  def set_model(self, model):\n",
        "    self.model = model\n",
        "    layer_outputs = [layer.output for layer in model.layers]\n",
        "    self.activations_model = Model(model.input, layer_outputs)\n",
        "\n",
        "  list_batch_acc = []\n",
        "  list_batch_loss = []\n",
        "  list_epoch_val_acc = []\n",
        "  list_epoch_val_loss = []\n",
        "\n",
        "  def on_train_batch_end(self, batch, logs=None):\n",
        "    GraphCallback.list_batch_acc.append(logs[\"accuracy\"])\n",
        "    GraphCallback.list_batch_loss.append(logs[\"loss\"])\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "    GraphCallback.list_epoch_val_acc.append(logs[\"val_accuracy\"])\n",
        "    GraphCallback.list_epoch_val_loss.append(logs[\"val_loss\"])\n",
        "    \n",
        "    if epoch > 1:\n",
        "\n",
        "      fig_1, axs_1 = pyplot.subplots(1, 2, figsize=(20, 5))\n",
        "\n",
        "      epochs_batch = ceil(Train_set.n / batch_size)\n",
        "      list_epoch = []\n",
        "      for i in range(0,len(GraphCallback.list_epoch_val_acc)):\n",
        "        list_epoch.append((i+1)*epochs_batch)\n",
        "      \n",
        "      list_ticks = []\n",
        "      for i in range(0,len(list_epoch)):\n",
        "        list_ticks.append(str(i+1))\n",
        "      \n",
        "      axs_1[0].plot(GraphCallback.list_batch_acc)\n",
        "      axs_1[0].plot(list_epoch,GraphCallback.list_epoch_val_acc)\n",
        "      axs_1[0].set_xticks(list_epoch)\n",
        "      axs_1[0].set_xticklabels(list_ticks)\n",
        "      axs_1[0].set_title(\"Model Accuracy\")\n",
        "      axs_1[0].set_xlabel(\"Epoch\")\n",
        "      axs_1[0].set_ylabel(\"Accuracy\")\n",
        "      axs_1[0].legend([\"Train\",\"Validation\"])\n",
        "\n",
        "      axs_1[1].plot(GraphCallback.list_batch_loss)\n",
        "      axs_1[1].plot(list_epoch,GraphCallback.list_epoch_val_loss)\n",
        "      axs_1[1].set_xticks(list_epoch)\n",
        "      axs_1[1].set_xticklabels(list_ticks)\n",
        "      axs_1[1].set_title(\"Model Loss\")\n",
        "      axs_1[1].set_xlabel(\"Epoch\")\n",
        "      axs_1[1].set_ylabel(\"Loss\")\n",
        "      axs_1[1].legend([\"Train\",\"Validation\"])\n",
        "    \n",
        "      pyplot.show()\n",
        "\n",
        "graphcallback = GraphCallback()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mScKJHrt_ZFv"
      },
      "source": [
        "## 6. Fine-tuning the model (*Optional*)\n",
        "---\n",
        "*If you don't want to train the network, skip this section and go to:* **7. Load a trained network**\n",
        "\n",
        "Start the training of our model with a maximum of 50 epochs and validating the trained net in every epoch with the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqJOruGR-khS",
        "cellView": "form"
      },
      "source": [
        "# @title 6.1 Start the fine-tuning\n",
        "model_history = model.fit_generator(Train_set, steps_per_epoch = ceil(Train_set.n / batch_size) ,epochs=150, shuffle=True,\n",
        "                                    validation_data=Validation_set, validation_steps = ceil(Validation_set.n / batch_size),\n",
        "                                    callbacks=[stopping, Learning_rate_decrease, checkpointer, graphcallback], verbose=2,\n",
        "                                    class_weight=class_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wryYj8QCm4wu",
        "cellView": "form"
      },
      "source": [
        "# @title 6.2 Visualize Final Loss and Accuracy\n",
        "# Loss plot\n",
        "pyplot.plot(model_history.history[\"loss\"])\n",
        "pyplot.plot(model_history.history[\"val_loss\"])\n",
        "pyplot.title(\"Model loss\")\n",
        "pyplot.ylabel(\"Loss\")\n",
        "pyplot.xlabel(\"Epoch\")\n",
        "pyplot.legend([\"Training\",\"Validation\"], loc=\"upper left\")\n",
        "pyplot.show()\n",
        "\n",
        "# ACC plot\n",
        "pyplot.plot(model_history.history[\"accuracy\"])\n",
        "pyplot.plot(model_history.history[\"val_accuracy\"])\n",
        "pyplot.title(\"Model accuracy\")\n",
        "pyplot.ylabel(\"Accuracy\")\n",
        "pyplot.xlabel(\"Epoch\")\n",
        "pyplot.legend([\"Training\",\"Validation\"], loc=\"upper left\")\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMDKjBENdit5"
      },
      "source": [
        "## 7. Load a trained network\n",
        "---\n",
        "*Skip this activity if you have fine-tuned your network. Go to:* **8. Performance assessment** \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XBywOQde41l",
        "cellView": "form"
      },
      "source": [
        "#@title 7.1 Load the pre-trained network\n",
        "#@markdown Load a ResNet-50 CNN fine-tuned on the ISIC 2019 dataset.\n",
        "with open(\"/content/models_and_metadata_isic_images/trained_CNN_case1/model_architecture.json\", 'r') as json_file:\n",
        "    json_savedModel= json_file.read()\n",
        "\n",
        "model = tf.keras.models.model_from_json(json_savedModel)\n",
        "#model_j.summary()\n",
        "\n",
        "#\n",
        "model.load_weights(\"/content/models_and_metadata_isic_images/trained_CNN_case1/model_weights.h5\")\n",
        "model.compile(optimizer='SGD', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jmXD1qc9YOSb"
      },
      "source": [
        "#@title 7.2 Visualize the CNN architecture\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHpFrshwftpH",
        "cellView": "form"
      },
      "source": [
        "#@title 7.3 Show performance curves { form-width: \"30%\" }\n",
        "#@markdown Display the evolution of accuracy and losses for the training and validation dataset.\n",
        "with open('/content/models_and_metadata_isic_images/trained_CNN_case1/trainHistoryDict', 'rb') as file_pi:\n",
        "      history=pickle.load(file_pi)\n",
        "\n",
        "fig_1, axs_1 = pyplot.subplots(1, 2, figsize=(20, 5))\n",
        "axs_1[0].plot(history['acc'])\n",
        "axs_1[0].plot(history['val_acc'])\n",
        "axs_1[0].set_ylabel('accuracy')\n",
        "axs_1[0].set_xlabel('epoch')\n",
        "axs_1[0].legend(['train', 'val'], loc='upper left')\n",
        "\n",
        "axs_1[1].plot(history['loss'])\n",
        "axs_1[1].plot(history['val_loss'])\n",
        "axs_1[1].set_ylabel('loss')\n",
        "axs_1[1].set_xlabel('epoch')\n",
        "axs_1[1].legend(['train', 'val'], loc='upper left')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1tU_MywVK3n",
        "cellView": "form"
      },
      "source": [
        "#@title 7.4 Build a new test set { form-width: \"30%\" }\n",
        "#@markdown Since we are using a model trained on the same dataset, we must ensure that the test images were not used for the training\n",
        "\n",
        "!rm -rf \"ISIC_2019\"\n",
        "\n",
        "newdir=\"ISIC_2019\"\n",
        "!mkdir $newdir\n",
        "\n",
        "newdir=\"ISIC_2019/test\"\n",
        "!mkdir $newdir\n",
        "\n",
        "newdir=\"ISIC_2019/test/benign\"\n",
        "!mkdir $newdir\n",
        "\n",
        "newdir=\"ISIC_2019/test/malignant\"\n",
        "!mkdir $newdir\n",
        "\n",
        "#@markdown Set the number of images for the new test set\n",
        "test_img  =  40  #@param{type: 'integer'} \n",
        "\n",
        "#csvf_BE = csvf[csvf[\"benign_malignant\"] == \"benign\"]\n",
        "#csvf_MAL = csvf[csvf[\"benign_malignant\"] == \"malignant\"]\n",
        "\n",
        "def download_newtest_dataset(subset,lesion, test_img):\n",
        "  with open ('/content/models_and_metadata_isic_images/metadata/'+lesion+'_test.txt', 'rb') as fp:\n",
        "    list_1 = pickle.load(fp)\n",
        "  savePath = \"ISIC_2019/\"\n",
        "  savePathTest = os.path.join(savePath, \"test\", lesion)\n",
        "  if len(list_1)>test_img:\n",
        "    ind=random.sample(list(list_1), test_img)\n",
        "  else: ind=list(list_1)\n",
        "  print(\"Downloading images of the class labeled as \" + lesion + \".\")\n",
        "  print(\"..........\")\n",
        "  for i in ind:\n",
        "     key = subset[\"isic_id\"].keys()[i]\n",
        "     name_img = subset[\"isic_id\"][key]\n",
        "     img_id = imageList[key][\"_id\"]\n",
        "     imageFileResp = api.get('image/%s/download' % img_id)\n",
        "     imageFileResp.raise_for_status()\n",
        "     imageFileOutputPath = os.path.join(savePathTest, '%s.jpg' % name_img)\n",
        "     with open(imageFileOutputPath, 'wb') as imageFileOutputStream:\n",
        "       for chunk in imageFileResp:\n",
        "         imageFileOutputStream.write(chunk)\n",
        "\n",
        "download_newtest_dataset(csvf_BE,'benign', test_img)\n",
        "download_newtest_dataset(csvf_MAL, 'malignant', test_img)\n",
        "print(\"Done.\")\n",
        "\n",
        "\n",
        "datagenValTest = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "#\n",
        "\n",
        "Test_set = datagenValTest.flow_from_directory(test_fold,\n",
        "                                target_size = inputSize[0:2],\n",
        "                                class_mode=\"categorical\", shuffle=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_AMWI1S_dEJ"
      },
      "source": [
        "## 8. Performance assessment\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Use the test dataset, that contains different images with respect to those used for the training and validation, to evaluate the performance of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Saoi1MMJ0cBq",
        "cellView": "form"
      },
      "source": [
        "#@title 8.1 Predictions { form-width: \"30%\" }\n",
        "#@markdown Obtain a prediction (benign/malignant) for each image of the test datset\n",
        "#Load Test_set images and labels\n",
        "x_Test,y_Test =[],[]\n",
        "for i in range(ceil(Test_set.n/Test_set.batch_size)):\n",
        "  x_test,y_test = Test_set.next()\n",
        "  x_Test.extend(x_test)\n",
        "  y_Test.extend(y_test)\n",
        "x_Test=np.array(x_Test)  \n",
        "y_Test=np.array(y_Test) \n",
        "Predictions = model.predict(x_Test)\n",
        "#One hot vector encoding for Test_set labels\n",
        "One_Hot_vector = np.argmax(y_Test,1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lam8WkuKZIV7",
        "cellView": "form"
      },
      "source": [
        "#@title 8.2 Show example of test images with ground-truth and corresponding prediction { form-width: \"30%\" }\n",
        "k=np.random.randint(Test_set.n)\n",
        "class_label=['benign','malignant']\n",
        "fig_ = pyplot.figure(1)\n",
        "chart_1 = fig_.add_subplot()\n",
        "\n",
        "#img = x_Test[k]\n",
        "img = Image.open(os.path.join('ISIC_2019','test',Test_set.filenames[k]))\n",
        "chart_1.imshow(img)\n",
        "chart_1.title.set_text('Ground truth = '+ class_label[int(One_Hot_vector[k])] + ';  Prediction = ' + class_label[np.argmax(Predictions[k])])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtyDW0T3-kj7",
        "cellView": "form"
      },
      "source": [
        "# @title 8.3 Accuracy and confusion matrix { form-width: \"30%\" }\n",
        "# Calculate accuracy over the test dataset\n",
        "#Test_set accuracy\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score\n",
        "\n",
        "#Test_acc = model.evaluate_generator(Test_set)[1]\n",
        "Test_acc = accuracy_score(np.argmax(y_Test, axis = 1), np.argmax(Predictions, axis = 1))\n",
        "print('The accuracy calculated over the ' + str(Test_set.n) + ' images composing the test set is equal to ' + str(np.round(Test_acc*100,0)) + '%' )\n",
        "\n",
        "Conf_mat = confusion_matrix(np.argmax(y_Test, axis = 1), np.argmax(Predictions, axis = 1))\n",
        "disp = ConfusionMatrixDisplay(Conf_mat,Test_set.class_indices.keys())\n",
        "disp.plot()\n",
        "disp.figure_.set_size_inches(5, 5)\n",
        "pyplot.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDVDUyXn-ktu",
        "cellView": "form"
      },
      "source": [
        "# @title 8.4 ROC curve and AUC\n",
        "\n",
        "# @markdown Plot the ROC curve using the labels of the test dataset and the predictions made by the model and calculate the AUC.\n",
        "#Calculate and plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_Test[:,0], Predictions[:,0])\n",
        "#Calculate and print AUC\n",
        "AUC = auc(fpr, tpr)\n",
        "\n",
        "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
        "pyplot.plot(fpr, tpr, marker='.')\n",
        "pyplot.xlabel('False Positive Rate')\n",
        "pyplot.ylabel('True Positive Rate')\n",
        "pyplot.text(0.7,0.2,'AUC = ' +str(np.round(AUC,3)) )\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w28avcMIxn0S",
        "cellView": "form"
      },
      "source": [
        "#@title 8.5 Visualize features generated at a specific layer  { form-width: \"30%\" }\n",
        "im = x_Test[k] #choose an image\n",
        "#@markdown Choose a layer between 0 and 174 { form-width: \"10%\" }\n",
        "layer_number =  3#@param{type: 'integer'}\n",
        "\n",
        "# Original image\n",
        "#pyplot.imshow(im)\n",
        "\n",
        "# Adding a dimension\n",
        "im = np.expand_dims(im, axis=0)\n",
        "layer_name = 'my_layer'\n",
        "intermediate_layer_model = tf.keras.Model(inputs=model.input,outputs=model.get_layer(index=layer_number).output)\n",
        "Layer_pred = intermediate_layer_model.predict(im)\n",
        "\n",
        "n_feat_to_show=np.min([Layer_pred.shape[-1], 36])\n",
        "\n",
        "subplot_X = int( n_feat_to_show**(1/2) ) \n",
        "subplot_Y = int( n_feat_to_show**(1/2) )\n",
        "fig = pyplot.figure(figsize=(15,15) )\n",
        "fig.suptitle('Plot of %d of the %d  features produced at layer no. %d.'%(n_feat_to_show,Layer_pred.shape[-1],layer_number), y=0.9 )\n",
        "for i in range(n_feat_to_show):#range(Layer_pred.shape[-1]):\n",
        "  sub = fig.add_subplot(subplot_X,subplot_Y, i + 1) #change for optimal display based on number of images\n",
        "  sub.imshow(Layer_pred[0,:,:,i], cmap='gray')\n",
        "pyplot.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}